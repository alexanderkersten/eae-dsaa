{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexanderkersten/eae-dsaa/blob/main/unsupervised.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbsqBHGhx56n"
      },
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzcmuNsfx56p"
      },
      "source": [
        "Unsupervised algorithms are those whose training data consists of a set of input variables $X$ without a target variable $Y$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dU1zv8U6x56p"
      },
      "source": [
        "There are two categories:\n",
        "\n",
        "* **Clustering**: discover groups with similar features within a dataset.\n",
        "\n",
        "* **Dimensionality reduction**: reduce a dataset with a high number of dimensions to two or three ones. This reduction will allow the visualization as well as a better knowledge about your data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "othWbSYHx56p"
      },
      "source": [
        "### Applications"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOIh6P0px56q"
      },
      "source": [
        "* **Customer segmentation**: the market is divided into smaller segments of buyers who have different needs, characteristics and behaviors to apply different strategies. **Note:** You could apply a customer segmentation using the CRM exercise based on $age$, $GDP$, $gender$, $ first$ $purchase$, $visits$, .etc.\n",
        "\n",
        "![](https://i.pinimg.com/originals/d7/2f/7b/d72f7bde33d814881a5d058212228514.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKv5Mcilx56q"
      },
      "source": [
        "* **Fraud detection**: identify which transactions can be considered false pretenses. It is about finding anomalous behaviours which are not related to the general behaviour of the rest of the population.\n",
        "\n",
        "Kaggle example:\n",
        "http://archive.ics.uci.edu/ml/datasets/statlog+(australian+credit+approval)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mz8faeEyx56r"
      },
      "source": [
        "* **Face detection using PCA**: principal component analysis is used to reduce the number of variables. The data is compressed in such a way that the main characteristics are preserved. In the case of an image where a face appears, we know that not all the pixels represent the main features of the face. Using PCA, we extract the main ones which define a face and reduce dimensions.  \n",
        "\n",
        "PCA example from scratch to detect faces:\n",
        "\n",
        "https://medium.com/@reubenrochesingh/building-face-detector-using-principal-component-analysis-pca-from-scratch-in-python-1e57369b8fc5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gc_I375vx56r"
      },
      "source": [
        "# Data generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2yZVbGNx56r"
      },
      "source": [
        "*K-means* technique is an unsupervised algorithm from *Clustering* category.\n",
        "\n",
        "Its purpose is to partition a set of $n$ observations into $k$ groups where each observation belongs to the group whose **mean value is the closest**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9SpjKzRRx56l"
      },
      "outputs": [],
      "source": [
        "#from IPython.display import Image\n",
        "#from IPython.core.display import HTML\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns # for plot styling\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtKp0Xxzx56s"
      },
      "source": [
        "We create a two dimensional example using one of the $sklearn$ library component called $make\\_blobs$ to generate clusters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9DdRE-xZx56s"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_blobs\n",
        "\n",
        "X, y = make_blobs(n_samples=300, centers=3, cluster_std=0.50, random_state=0)  # Generate example with 300 points and 3 centers\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], s=50)  # Scatter plot to visualize it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vISlyqvDx56t"
      },
      "outputs": [],
      "source": [
        "help(make_blobs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vND8cd4wx56u"
      },
      "source": [
        "We can see that the number of groups are 3. The ${K-means}$ algorithm detects automatically to which group each data point has to be assigned:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKSmTIyKx56u"
      },
      "source": [
        "# Model training\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BYnYunOQx56v"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans # Load kmeans class\n",
        "\n",
        "kmeans = KMeans(n_clusters=3)      # Number of groups are pre-defined\n",
        "kmeans.fit(X)                      # Training"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model inference"
      ],
      "metadata": {
        "id": "qUGcYGEF5Sgg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "udhg5edKx56v"
      },
      "outputs": [],
      "source": [
        "y_predicted = kmeans.predict(X)       # Prediction\n",
        "y_predicted"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjFb28EKx56v"
      },
      "source": [
        "## What is happening\n",
        "\n",
        "Each of the 300 points has been associated with one of the three previously set groups:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualize how k-means works"
      ],
      "metadata": {
        "id": "in-wMYb65h5x"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPHHgYO-x56w"
      },
      "source": [
        "## Figure out the center positions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ynCuVHDUx56w"
      },
      "outputs": [],
      "source": [
        "centers = kmeans.cluster_centers_\n",
        "\n",
        "print(centers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9bLQwKDx56w"
      },
      "source": [
        "We represent the points with the colour associated to their specific group. We also plot the centroids groups which are defined as the minimum mean distance of each set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pQN3GBlRx56x"
      },
      "outputs": [],
      "source": [
        "plt.scatter(X[:, 0], X[:, 1], c=y_predicted, s=50, cmap='viridis')\n",
        "plt.scatter(centers[:, 0], centers[:, 1], c='red', s=50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TISzfu1gx56x"
      },
      "source": [
        "$K-means$ algorithm assigns the points to the clusters as you would have done.\n",
        "\n",
        "The entire point is knowing how it works. The good news is this methodology is very simple and we could implement it by ourselves. This method is based on the **Expectation-Maximization algorithm** and the approach consists on:\n",
        "\n",
        "* 1. Estimate the centroids\n",
        "* 2. Repeat the process until convergence is reached:\n",
        "        * *Expectation step*: Assign the points to the closest cluster.\n",
        "        * *Maximization step*: Set the centroids based on the new computed mean."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Py82o3I1x56y"
      },
      "source": [
        "See the following code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "UAKRDV2Sx56y"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import pairwise_distances_argmin\n",
        "import math\n",
        "\n",
        "def find_clusters(X, centers):\n",
        "    \"\"\"\n",
        "    Find the clusters within a dataset\n",
        "    :param iterable X: datasets\n",
        "    :param iterable centers: initial solution\n",
        "    \"\"\"\n",
        "    # Initial parameters\n",
        "    iters = 0\n",
        "    n_clusters = len(centers)                    # Number of clusters\n",
        "    centers_iters = []                           # Save centers for each iteration\n",
        "    labels_iters = []                            # Save assigments for each iteration\n",
        "\n",
        "    while True:\n",
        "        # Assign the points to the closest group\n",
        "        labels = pairwise_distances_argmin(X, centers)\n",
        "\n",
        "        # Save results\n",
        "        centers_iters.append(centers)\n",
        "        labels_iters.append(labels)\n",
        "\n",
        "        # Reallocate the centroids\n",
        "        new_centers = np.array([X[labels == i].mean(0)\n",
        "                                for i in range(n_clusters)])\n",
        "\n",
        "        # Check convergence\n",
        "        # In this case we're forcing the function to reach the same center as the cluster\n",
        "        if np.all(centers == new_centers):\n",
        "            break\n",
        "\n",
        "        centers = new_centers\n",
        "        iters += 1\n",
        "    return centers_iters, labels_iters, iters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "g__fyY8lx56z"
      },
      "outputs": [],
      "source": [
        "def visualize_kmeans_process(centers_iters, labels_iters, n_clusters, iters):\n",
        "    \"\"\"\n",
        "    Visualize the kmeans process for each iteration\n",
        "    :param iterable centers_iters: centers for each iteration\n",
        "    :param iterable labels: assigments for each iteration\n",
        "    :param int n_clusters: number of clusters\n",
        "    :param int iters: number of iteration until finding the convergence\n",
        "    \"\"\"\n",
        "    columns = iters+1\n",
        "    iters = 0\n",
        "    fig, axs = plt.subplots(1, columns, figsize=(20, 6))\n",
        "    for col in range(0, columns):\n",
        "        axs[col].scatter(X[:, 0], X[:, 1], c=labels_iters[iters], s=50, cmap='viridis')\n",
        "        for cluster in range(n_clusters):\n",
        "            axs[col].scatter(centers_iters[iters][cluster][0],\n",
        "                                  centers_iters[iters][cluster][1], c='red', s=50);\n",
        "        axs[col].set_title(\"Iter \" + str(iters))\n",
        "        iters += 1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "centers = np.array([[1, 1], [2, 3], [2, 1]])\n",
        "#centers = np.array([[1, 1], [1, 3], [2, 1]])\n",
        "\n",
        "centers_iters, labels_iters, iters = find_clusters(X, centers)\n",
        "\n",
        "n_clusters = len(centers)\n",
        "visualize_kmeans_process(centers_iters, labels_iters, n_clusters, iters)"
      ],
      "metadata": {
        "id": "M4w1wJhoBtKn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f3tujY8Px560"
      },
      "outputs": [],
      "source": [
        "help(kmeans)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZntE_wFx560"
      },
      "source": [
        "The first graph shows an initial cluster assignment that is not the desired one because of the random centroids used.\n",
        "\n",
        "However, the centroids are getting closer to their corresponding groups until the solution coverges. **It happens when the distance of the points to the closest centroid does not produce new assigments**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXN5j4-ax560"
      },
      "source": [
        "## Does the result depend on the initial centroids?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vgMQWjKax561"
      },
      "outputs": [],
      "source": [
        "centers = np.array([[2, 0], [3, 1], [2, 1]])\n",
        "\n",
        "centers_iters, labels_iters, iters = find_clusters(X, centers)\n",
        "\n",
        "n_clusters = len(centers)\n",
        "visualize_kmeans_process(centers_iters, labels_iters, n_clusters, iters)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAtg6ql-x561"
      },
      "source": [
        "## Using only two groups"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gsD7Kb24x561"
      },
      "outputs": [],
      "source": [
        "centers = np.array([[1, 1], [2, 3]])\n",
        "\n",
        "centers_iters, labels_iters, iters = find_clusters(X, centers)\n",
        "\n",
        "n_clusters = len(centers)\n",
        "visualize_kmeans_process(centers_iters, labels_iters, n_clusters, iters)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4640GP1x569"
      },
      "source": [
        "# Limitations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOC6zSv4x569"
      },
      "source": [
        "## Number of clusters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJT-SYpJx569"
      },
      "source": [
        "One of the most important limitations is that $K-means$ needs the number of groups as an argument. How are we going to know a priori the number of groups if we want to use this method to figure it out?\n",
        "\n",
        "What happen if we had chosen a different number of clusters?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8NBqNodIx56-"
      },
      "outputs": [],
      "source": [
        "kmeans = KMeans(n_clusters=5)      # The number of clusters works an argument\n",
        "kmeans.fit(X)                      # Training\n",
        "y_kmeans = kmeans.predict(X)       # Prediction\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='viridis')\n",
        "centers = kmeans.cluster_centers_\n",
        "plt.scatter(centers[:, 0], centers[:, 1], c='red', s=50);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhJNrP6Rx56-"
      },
      "source": [
        "To solve this problem, we can execute multiple $K-means$ with different number of groups and choose the one which meets a certain criteria. There are several criteria that allow us to measure \"how well\" the clusters have achieved. The two most famous criteria are the $Elbow$  and the $Silhouette$ methods. For more information, see the following link:\n",
        "\n",
        "https://medium.com/analytics-vidhya/how-to-determine-the-optimal-k-for-k-means-708505d204eb\n",
        "\n",
        "We use $Silhoutte$ for two reasons:\n",
        "\n",
        "* Already implemented in $sklearn$.\n",
        "* The optimum number of groups is automatically extracted.\n",
        "\n",
        "**This criteria is based on the idea that a point will belong to a group if it is very close to its centroid and very far from the another ones.**\n",
        "\n",
        "See the following code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eAziFhjCx56-"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "scores = []\n",
        "groups = np.arange(2, 11)  # 2, 3, 4, ..., 8, 9, 10\n",
        "\n",
        "for k in groups:\n",
        "  kmeans = KMeans(n_clusters = k, n_init = 10).fit(X)\n",
        "  labels = kmeans.labels_\n",
        "  scores.append(silhouette_score(X, labels, metric = 'euclidean'))\n",
        "\n",
        "plt.plot(groups, scores)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhjVTTwBx56_"
      },
      "source": [
        "The maximum value is $K=6$"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Check Silhouette analysis](https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html) from scikit-learn"
      ],
      "metadata": {
        "id": "PSDOmplhIXkk"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSyeOGSkx56_"
      },
      "source": [
        "## Lineal Separation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y33JKU8lx56_"
      },
      "source": [
        "The fundamental model assumptions of k-means (points will be closer to their own cluster center than to others) means that the algorithm will often be ineffective if the clusters have complicated geometries.\n",
        "\n",
        "In particular, the boundaries between k-means clusters will always be linear, which means that it will fail for more complicated boundaries. Consider the following data, along with the cluster labels found by the typical k-means approach:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ph0noo2Vx57A"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_circles\n",
        "X, y = make_circles(n_samples=400, factor=.3, noise=.05)\n",
        "plt.scatter(X[:, 0], X[:, 1], s=50, cmap='viridis')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PI_ZsonIx57A"
      },
      "outputs": [],
      "source": [
        "help(make_circles)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5xqhKMlx57A"
      },
      "source": [
        "### Do the groups above have a linear separation?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v1MsSUAxx57B"
      },
      "outputs": [],
      "source": [
        "# Use Kmeans from sklearn using 2 clusters\n",
        "\n",
        "kmeans = KMeans(n_clusters = 2).fit(X)\n",
        "y_kmeans = kmeans.predict(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bR3RMhndx57B"
      },
      "outputs": [],
      "source": [
        "plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='viridis')\n",
        "centers = kmeans.cluster_centers_\n",
        "plt.scatter(centers[:, 0], centers[:, 1], c='red', s=50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5D9QpjMex57C"
      },
      "source": [
        "In order to solve this, we can use a kernel transformation to project the data into a higher dimension where a linear separation is possible. We might imagine using the same trick to allow k-means to discover non-linear boundaries.\n",
        "\n",
        "One version of this kernelized k-means is implemented in Scikit-Learn within the $SpectralClustering$ estimator. It uses the graph of nearest neighbors to compute a higher-dimensional representation of the data, and then assigns labels using a k-means algorithm:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HBFYl0ZAx57C"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import SpectralClustering\n",
        "model = SpectralClustering(n_clusters=2, affinity='nearest_neighbors', assign_labels='kmeans')\n",
        "labels = model.fit_predict(X)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels, s=50, cmap='viridis');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YElB5Ghzx57C"
      },
      "source": [
        "**In real cases**, it's hard to check if your clusters have a linear separation because of the number of dimensions. The approach will be to try different models and see how it works based on your requirements."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0YKDe1zx57D"
      },
      "source": [
        "## Categorical Variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2a8NUYhx57D"
      },
      "source": [
        "The standard k-means algorithm isn't directly applicable to categorical data, for various reasons. The sample space for categorical data is discrete, and doesn't have a natural origin. A Euclidean distance function on such a space isn't really meaningful.\n",
        "\n",
        "There's a variation of k-means known as k-modes, introduced in this paper by Zhexue Huang, which is suitable for categorical data:\n",
        "\n",
        "http://www.cs.ust.hk/~qyang/Teaching/537/Papers/huang98extensions.pdf\n",
        "\n",
        "An example in python:\n",
        "\n",
        "https://towardsdatascience.com/the-k-prototype-as-clustering-algorithm-for-mixed-data-type-categorical-and-numerical-fe7c50538ebb"
      ]
    }
  ],
  "metadata": {
    "hide_input": false,
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}