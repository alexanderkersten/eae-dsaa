{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexanderkersten/eae-dsaa/blob/main/decision_tree.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0If0kpZxb-7"
      },
      "source": [
        "# 1. Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- seaborn (0.13.1)\n",
        "- pydotplus (2.0.2)\n",
        "- scikit-learn (1.2.2)\n",
        "- pandas (2.0.3)\n",
        "- dtreeviz (2.2.2)\n",
        "- graphviz (0.20.3)\n",
        "- matplotlib (3.7.1)"
      ],
      "metadata": {
        "id": "pCyxvbJC19EI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VB8Jgi0exb-8"
      },
      "outputs": [],
      "source": [
        "!pip install dtreeviz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hzqAj4hJxb_A"
      },
      "outputs": [],
      "source": [
        "import sklearn.datasets as datasets\n",
        "\n",
        "iris = datasets.load_iris(as_frame=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Analysis"
      ],
      "metadata": {
        "id": "Z4OlP-ipNPPt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "type(iris)"
      ],
      "metadata": {
        "id": "ECLbw_2oNQzr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRmeYXTBxb_B"
      },
      "outputs": [],
      "source": [
        "print(iris.DESCR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MmEXImDCxb_C"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "X = iris.data\n",
        "y = iris.target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqQnBUc4xb_M"
      },
      "source": [
        "If you notice the name of the variables above, you'll see the X's and y's. Y is that?\n",
        "\n",
        "In math notation, we define the dataset X as the array of feature vectors, each vector representing the description of a flower in our dataset, each feature representing an aspect of a flower). With all these values in X, we want to infer an approximate function $\\hat{y}$, using the labeled dataset in variable `y` to better **fit** the data in `X`.\n",
        "\n",
        "Or $\\hat{y} = f(X)$\n",
        "\n",
        "Here we also use 'y' as the name of the variable that holds the target variable (species)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X.head()"
      ],
      "metadata": {
        "id": "QKR92gneNdB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Nh8DGvjxb_C"
      },
      "outputs": [],
      "source": [
        "y.to_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hNSxhHBrxb_C"
      },
      "outputs": [],
      "source": [
        "y.hist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3V3WQCOLxb_D"
      },
      "outputs": [],
      "source": [
        "iris.target_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xd34Iu-1xb_D"
      },
      "outputs": [],
      "source": [
        "iris_df = X.copy()\n",
        "iris_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F95x6ktmxb_F"
      },
      "outputs": [],
      "source": [
        "species_dict = {0: 'setosa', 1: 'versicolor', 2: 'virginica'}\n",
        "iris_df['species'] = y.map(species_dict)\n",
        "iris_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qUvPDI_kxb_G"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "\n",
        "sns.pairplot(iris_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pOL0JPI6xb_H"
      },
      "outputs": [],
      "source": [
        "sns.pairplot(iris_df, hue='species', plot_kws=dict(alpha=0.3))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Model training"
      ],
      "metadata": {
        "id": "SNfxczbsR9WY"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9pVAPmuxb_J"
      },
      "source": [
        "## Dividing the dataset into train and test datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DencIJNZxb_J"
      },
      "source": [
        "We are going to create a decision tree with a partition between train and test of the data. Look at the variable *random_state* that is applied in the `train_test_split` function. By changing this variable, the random distribution between the train and test data will change."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EhruiWWjxb_J"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-RbT1ozbxb_J"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RV-FHmzkxb_K"
      },
      "outputs": [],
      "source": [
        "len(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BHa92jsNxb_K"
      },
      "outputs": [],
      "source": [
        "len(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkyB3x8gxb_K"
      },
      "source": [
        "## Why dividing the dataset is important?\n",
        "\n",
        "You want your model to generalize well, so sampling is important and you need to avoid data leakage, i.e. using the training dataset as the test dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvYTr5ySxb_L"
      },
      "source": [
        "Additional resources\n",
        "\n",
        "- [K Fold Cross Validation](https://www.section.io/engineering-education/how-to-implement-k-fold-cross-validation/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8TY3cxdxb_L"
      },
      "source": [
        "## Training a Decision Tree Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_OEqkxexb_H"
      },
      "source": [
        "In decision trees we're creating structures like:\n",
        "![](https://miro.com/blog/wp-content/uploads/2021/12/decision_tree_business_analysis.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_M-yO-7Dxb_L"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3npPOcTLxb_L"
      },
      "outputs": [],
      "source": [
        "# Create the model\n",
        "dtree = DecisionTreeClassifier()\n",
        "\n",
        "# Train the model\n",
        "# X_train contains the feature vectors for examples of flowers\n",
        "# y_train contains the classes of these flowers\n",
        "dtree.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXjLjn5sxb_L"
      },
      "source": [
        "## Exporting model to file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8mZZ6bhxb_M"
      },
      "source": [
        "This can be used to create a flower classifier application, for example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CgAxLynkxb_M"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "# Saves file (serialized dtree object)\n",
        "# wb stands for Write Binary\n",
        "pickle.dump(dtree, open('dtree_iris.pkl', 'wb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m2jKDTy8xb_M"
      },
      "outputs": [],
      "source": [
        "# Just showing the file was actually created\n",
        "!ls | grep pkl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0t2P3e2ixb_N"
      },
      "source": [
        "## Visualizing resulting decision tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1GyoMuhOxb_N"
      },
      "outputs": [],
      "source": [
        "import pydotplus\n",
        "from io import StringIO\n",
        "from IPython.display import Image\n",
        "from sklearn.tree import export_graphviz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1YWVpyY1xb_N"
      },
      "outputs": [],
      "source": [
        "dot_data = StringIO()\n",
        "export_graphviz(dtree,\n",
        "                out_file=dot_data,\n",
        "                feature_names=iris.feature_names,\n",
        "                class_names=iris.target_names,\n",
        "                filled=True,\n",
        "                rounded=True,\n",
        "                special_characters=True)\n",
        "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())\n",
        "Image(graph.create_png())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQC01F0Lxb_I"
      },
      "source": [
        "**Decision Trees** or CART (Classification and Regression Trees):\n",
        "\n",
        "This method is based on the order of the data. It is, therefore, an iterative method in which each step will try to reduce the *impurity* of the data based on the variable to be predicted. Digging a bit:\n",
        "\n",
        "1. We start with all our data in the same bag, therefore I have all the different categories mixed together.\n",
        "By categories we mean the class, the target variable, i.e. what we're trying to predict in a classification task.\n",
        "\n",
        "    1. The impurity of this data bag is modeled using the **Gini impurity index** (not to be confused with the Gini coefficient).\n",
        "    1. Simplifying it, it could be seen as: If we introduce a new observation into our bag, whose response variable has been chosen based on the distribution of the different categories, what is the probability of being wrong if I try to classify it?\n",
        "    1. In other terms: if I have a sack with two pears and two oranges, the impurity is 50%, since a new variable could be with the same probability either pear or orange.\n",
        "\n",
        "2. Then, as we want to reduce this impurity, we will distribute the observations in different bags in each iteration based on the characteristics of our variables until we obtain groups where there is no possible error: Either everything is pears or everything is oranges.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8R_bARbxb_N"
      },
      "source": [
        "We can see that there is a variable that strongly marks a first cut, so that already in the first split we obtain a partition without impurity. It is a model capable of obtaining a very accurate division of the data in very few steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ICVbrvmkxb_O"
      },
      "outputs": [],
      "source": [
        "import dtreeviz\n",
        "import matplotlib as mpl\n",
        "\n",
        "viz = dtreeviz.model(dtree, X_train, y_train,\n",
        "               target_name=\"Species\",\n",
        "               feature_names=iris.feature_names,\n",
        "               class_names=list(iris.target_names))\n",
        "\n",
        "viz.view(fontname=\"DejaVu Sans\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dtree.feature_names_in_ = None"
      ],
      "metadata": {
        "id": "N4AejZ9esPvv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJH-TYzrxb_O"
      },
      "source": [
        "If we now repeat this process with a different distribution between train and test, which implies that the model has been trained with other data, by changing `random_state` to other value ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ttiA5CQnxb_P"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=60)\n",
        "\n",
        "dtree = DecisionTreeClassifier()\n",
        "dtree.fit(X_train, y_train)\n",
        "\n",
        "viz = dtreeviz.model(dtree, X_train, y_train,\n",
        "               target_name=\"Species\",\n",
        "               feature_names=iris.feature_names,\n",
        "               class_names=list(iris.target_names))\n",
        "\n",
        "viz.view(fontname=\"DejaVu Sans\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ir_5JlX9xb_P"
      },
      "outputs": [],
      "source": [
        "# Let's check the influence of the random_state variable\n",
        "help(train_test_split)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YL5ZTFiqxb_P"
      },
      "source": [
        "## Some words about overfitting and underfitting\n",
        "\n",
        "Sometimes you train a ML algorithm on the labeled dataset you have, but when faced with new unseen data the model starts performing bad, and therefore making wrong classifications. This happens because the model hasn't generalized the data patterns correctly. It was only replicating the data distribution in a highly specialized or generic way.\n",
        "\n",
        "![](https://www.mathworks.com/discovery/overfitting/_jcr_content/mainParsys/image.adapt.full.medium.svg/1705396624275.svg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_Ddt_3kxb_P"
      },
      "source": [
        "## Train-validation-test split\n",
        "\n",
        "One way to try to cope with overfitting is doing a more complex strategy for dividing the dataset into:\n",
        "\n",
        "1. Train datased: used to train the model\n",
        "1. Validation (or development) dataset: used to optimize our model parameters to achieve best accuracy possible\n",
        "1. Test dataset: unseen data separated to test our model with possibly different patterns not found in previous datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C9CLIadCxb_Q"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, random_state=42\n",
        ")\n",
        "\n",
        "X_train, X_dev, y_train, y_dev = train_test_split(\n",
        "    X_train, y_train, test_size=0.10, random_state=42\n",
        ")\n",
        "\n",
        "# F-strings https://realpython.com/python-f-strings/\n",
        "print(f'Training size: {len(X_train)}, Validation size: {len(X_dev)}, Test size: {len(X_test)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zrOuItGhxb_Q"
      },
      "outputs": [],
      "source": [
        "# Create the model\n",
        "dtree = DecisionTreeClassifier()\n",
        "\n",
        "# Train the model\n",
        "dtree.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5D3zaEC_xb_Q"
      },
      "outputs": [],
      "source": [
        "viz = dtreeviz.model(dtree, X_train, y_train,\n",
        "               target_name=\"Species\",\n",
        "               feature_names=iris.feature_names,\n",
        "               class_names=list(iris.target_names))\n",
        "\n",
        "viz.view(fontname=\"DejaVu Sans\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mX7MiTlUxb_R"
      },
      "source": [
        "## Checking accuracy with the validation dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f8tabGgBxb_R"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "\n",
        "y_dev_predicted = dtree.predict(X_dev)\n",
        "y_dev_predicted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TK0Eb-nhxb_R"
      },
      "outputs": [],
      "source": [
        "print(f'Accuracy with validation dataset {accuracy_score(y_dev_predicted, y_predicted)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLAc2Jezxb_R"
      },
      "source": [
        "At this point we can do multiple runs of train/validation checks to improve the accuracy score..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKs-ZHx3xb_R"
      },
      "source": [
        "## Inferring over new unforeseen data (X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xhORvkeuxb_S"
      },
      "outputs": [],
      "source": [
        "# The same but with X_test, y_test\n",
        "y_test_predicted = dtree.predict(X_test)\n",
        "y_test_predicted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bVHtCPmcxb_S"
      },
      "outputs": [],
      "source": [
        "print(f'Accuracy with test dataset {accuracy_score(y_test_predicted, y_predicted)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7BGdsYcxb_S"
      },
      "source": [
        "## Decision trees for regression problems"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlS1tfHExb_T"
      },
      "source": [
        "### Diabetes dataset\n",
        "\n",
        "Target is a quantitative measure of disease progression one year after baseline\n",
        "\n",
        "![](https://explained.ai/decision-tree-viz/images/samples/diabetes-TD-3-X.svg)\n",
        "\n",
        "In the above example we can see Decision Trees create **LINEAR** decision boundaries for its variables.\n",
        "This is amazing to generate (visual) explanations for your resulting model BUT...\n",
        "\n",
        "What would happen if the feature vector distribution is like in the following image:\n",
        "![](https://media.geeksforgeeks.org/wp-content/uploads/20200605170732/linearsep.png)\n",
        "\n",
        "In this case we probably need Support Vector Machines (SVM), which are able to divide the feature space into curved decision boundaries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQ3rgCiSxb_T"
      },
      "source": [
        "## Additional reading\n",
        "\n",
        "- [A visual introduction to machine learning](http://www.r2d3.us/visual-intro-to-machine-learning-part-1/)\n",
        "- [Understanding Decision Trees with Python](https://datascience.foundation/sciencewhitepaper/understanding-decision-trees-with-python)\n",
        "- [4 Ways to Visualize Individual Decision Trees in a Random Forest](https://towardsdatascience.com/4-ways-to-visualize-individual-decision-trees-in-a-random-forest-7a9beda1d1b7)\n",
        "- [How to Visualize a Random Forest with Fitted Parameters?](https://analyticsindiamag.com/how-to-visualize-a-random-forest-with-fitted-parameters/)\n",
        "- [Understanding Random forest better through visualizations](https://garg-mohit851.medium.com/random-forest-visualization-3f76cdf6456f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EY64pWXfxb_U"
      },
      "source": [
        "## Improving the Model\n",
        "\n",
        "There are two very useful techniques that allow us to create new models:\n",
        "\n",
        "* **Bootstrap**: It is based on choosing subsamples of our data in a uniform way and with repetition, thus creating multiple smaller samples that share the distributions of the original sample.\n",
        "\n",
        "* **Bagging**: Generate a bootstrap of size $n$, train a model on that subsample and repeat the process $m$ times."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GW8zY4fjxb_U"
      },
      "source": [
        "## Random Forests\n",
        "\n",
        "Now let's apply an *ensemble* method such as *Random Forest*, based on the results of multiple decision trees.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "absn-PZLxb_U"
      },
      "source": [
        "There are other types of combining classifiers (or ensemble methods), such as mean average, or the product rule, using a priory probability distributions and the confidence of the classification of each classifier.\n",
        "\n",
        "Additional reading:\n",
        "\n",
        "- [Scikit Learn ensemble methods](https://scikit-learn.org/stable/modules/ensemble.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0DfdW7Axb_U"
      },
      "source": [
        "## Training a Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lm0Wiw4axb_V"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.33, random_state=100)\n",
        "\n",
        "rf = RandomForestClassifier(n_jobs=-1) # parallelize the execution\n",
        "rf.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhADPZftxb_V"
      },
      "source": [
        "Let's analyze the code ... ([RFs in Scikit-Learn](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J6B26aFYxb_V"
      },
      "outputs": [],
      "source": [
        "help(rf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfGyWbKrxb_V"
      },
      "source": [
        "## Real species"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RYm8_6PExb_V"
      },
      "outputs": [],
      "source": [
        "y_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8oy5Sr6xb_W"
      },
      "source": [
        "## Predicting the class of test dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gf6lrbPwxb_W"
      },
      "outputs": [],
      "source": [
        "predicted = rf.predict(X_test)\n",
        "predicted"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccH2FXOYxb_W"
      },
      "source": [
        "## Accuracy score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j5ZV_VuMxb_W"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sESEo-xPxb_X"
      },
      "outputs": [],
      "source": [
        "accuracy_score(y_test, predicted)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kt1SLP2Nxb_X"
      },
      "outputs": [],
      "source": [
        "help(accuracy_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlRX-1ouxb_X"
      },
      "source": [
        "## Feature importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3IyAbHtyxb_X"
      },
      "outputs": [],
      "source": [
        "rf = RandomForestClassifier(n_jobs=4)\n",
        "rf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wTU6nWANxb_Y"
      },
      "outputs": [],
      "source": [
        "rf.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xmXSL6bQxb_Y"
      },
      "outputs": [],
      "source": [
        "importances = rf.feature_importances_\n",
        "importances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CjmWAx9Gxb_Y"
      },
      "outputs": [],
      "source": [
        "indexes = np.argsort(importances)[::-1]\n",
        "indexes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0t3ddweQxb_Y"
      },
      "outputs": [],
      "source": [
        "X.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lQDole2Nxb_Y"
      },
      "outputs": [],
      "source": [
        "# Map importances to species_names\n",
        "names = [X.columns[i] for i in indexes]\n",
        "names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z674qMijxb_Z"
      },
      "outputs": [],
      "source": [
        "# Prepare a barplot\n",
        "plt.bar(range(X_train.shape[1]), importances[indexes])\n",
        "# Add the feature names\n",
        "plt.xticks(range(X_train.shape[1]), names, rotation=20, fontsize = 8)\n",
        "# Add the title\n",
        "plt.title(\"Feature Importance\")\n",
        "# Self explanatory...\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEscIEHBxb_Z"
      },
      "source": [
        "In the code of the upper cell we see not only how we can apply a model, but that it contains the importance of the different *features* or predictor variables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UghMh4TExb_Z"
      },
      "source": [
        "## Predict() function and confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dtnxETsnxb_Z"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "rf_preds = rf.predict(X_test)\n",
        "rf_conf_mat = confusion_matrix(y_test, rf_preds)\n",
        "rf_conf_mat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nHnjj_Dwxb_Z"
      },
      "outputs": [],
      "source": [
        "help(confusion_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHIJUdmLxb_a"
      },
      "source": [
        "You can get the accuracy score with data from confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JuKV7IWSxb_a"
      },
      "outputs": [],
      "source": [
        "# Convert to numpy\n",
        "np_mat = np.asarray(rf_conf_mat)\n",
        "\n",
        "acc = sum(np.diagonal(np_mat)) / np_mat.sum()\n",
        "print(f\"My accuracy is: {acc}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNyEDXYCxb_a"
      },
      "source": [
        "## Confusion matrix visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tHo-RbZFxb_a"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import ConfusionMatrixDisplay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JN3hkC7dxb_a"
      },
      "outputs": [],
      "source": [
        "disp = ConfusionMatrixDisplay(confusion_matrix=rf_conf_mat,display_labels=iris.target_names)\n",
        "disp.plot()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}